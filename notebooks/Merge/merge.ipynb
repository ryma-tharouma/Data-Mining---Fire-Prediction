{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84aa1932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "030faa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base paths\n",
    "data_dir = \"../../data/loaded\"\n",
    "processed_data_dir = \"../../data/processed\"\n",
    "Images_dir = \"../../images/processing_1\"\n",
    "processed_data_dir = \"../../data/processed\"\n",
    "merged_dir = \"../../data/merged\"\n",
    "soil_file_path = os.path.join(processed_data_dir, \"soil_reduced_nondup.csv\")\n",
    "land_file_path = os.path.join(processed_data_dir, \"landcover_processed.csv\")\n",
    "elev_file_path = os.path.join(data_dir, \"elevation.csv\")\n",
    "fire_file_path = os.path.join(processed_data_dir, \"fire_buffer1000.parquet\")\n",
    "clim_file_path = os.path.join(processed_data_dir, \"processed_climate.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21548d53",
   "metadata": {},
   "source": [
    "# join with landcover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d39fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------\n",
    "# Define paths\n",
    "# -----------------------------\n",
    "data_dir = \"../../data/loaded\"\n",
    "processed_data_dir = \"../../data/processed\"\n",
    "merged_dir = \"../../data/merged\"\n",
    "\n",
    "Path(merged_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "land_file_path = os.path.join(processed_data_dir, \"landcover_processed.csv\")\n",
    "fire_file_path = os.path.join(processed_data_dir, \"fire_buffer1000.parquet\")\n",
    "output_file = os.path.join(merged_dir, \"fire_with_landcover.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dce962e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fire\n",
       "0    89.558062\n",
       "1    10.441938\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire = pd.read_parquet(fire_file_path)\n",
    "fire['fire'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f8a2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPATIAL JOIN: Fire Points â†’ Landcover Polygons\n",
      "============================================================\n",
      "\n",
      "[1/4] Loading landcover data...\n",
      "Landcover polygons: 438,513\n",
      "\n",
      "[2/4] Creating latitude bands...\n",
      "Latitude bands: 2\n",
      "Coverage: [31.0 - 38.0]\n",
      "\n",
      "[3/4] Processing latitude bands...\n",
      "\n",
      "Band 1/2: Lat [30.95, 36.05] â†’ 35,116 pts | 140,981 polys | âœ“ 35,116 (100.0%) | 37.9s\n",
      "\n",
      "Band 2/2: Lat [35.95, 38.05] â†’ 8,200 pts | 59,922 polys"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T14s\\AppData\\Local\\Temp\\ipykernel_18448\\2701735899.py:129: UserWarning: Geometry is in a geographic CRS. Results from 'distance' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  nearest_idx = fire_unmatched_gdf.geometry.apply(lambda pt: landcover_filtered.distance(pt).idxmin())\n",
      "C:\\Users\\T14s\\AppData\\Local\\Temp\\ipykernel_18448\\2701735899.py:129: UserWarning: Geometry is in a geographic CRS. Results from 'distance' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  nearest_idx = fire_unmatched_gdf.geometry.apply(lambda pt: landcover_filtered.distance(pt).idxmin())\n",
      "C:\\Users\\T14s\\AppData\\Local\\Temp\\ipykernel_18448\\2701735899.py:129: UserWarning: Geometry is in a geographic CRS. Results from 'distance' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  nearest_idx = fire_unmatched_gdf.geometry.apply(lambda pt: landcover_filtered.distance(pt).idxmin())\n",
      "C:\\Users\\T14s\\AppData\\Local\\Temp\\ipykernel_18448\\2701735899.py:129: UserWarning: Geometry is in a geographic CRS. Results from 'distance' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  nearest_idx = fire_unmatched_gdf.geometry.apply(lambda pt: landcover_filtered.distance(pt).idxmin())\n",
      "C:\\Users\\T14s\\AppData\\Local\\Temp\\ipykernel_18448\\2701735899.py:129: UserWarning: Geometry is in a geographic CRS. Results from 'distance' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  nearest_idx = fire_unmatched_gdf.geometry.apply(lambda pt: landcover_filtered.distance(pt).idxmin())\n",
      "C:\\Users\\T14s\\AppData\\Local\\Temp\\ipykernel_18448\\2701735899.py:129: UserWarning: Geometry is in a geographic CRS. Results from 'distance' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  nearest_idx = fire_unmatched_gdf.geometry.apply(lambda pt: landcover_filtered.distance(pt).idxmin())\n",
      "C:\\Users\\T14s\\AppData\\Local\\Temp\\ipykernel_18448\\2701735899.py:129: UserWarning: Geometry is in a geographic CRS. Results from 'distance' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  nearest_idx = fire_unmatched_gdf.geometry.apply(lambda pt: landcover_filtered.distance(pt).idxmin())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | âœ“ 8,200 (100.0%) | 1.7s\n",
      "\n",
      "[4/4] Combining all bands...\n",
      "Before dedup: 43,316 rows\n",
      "After dedup: 42,291 rows\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Total fire points: 42,291\n",
      "With LCCCode: 42,291\n",
      "Without LCCCode: 0\n",
      "Match rate: 100.00%\n",
      "Total time: 0.7 minutes\n",
      "Output saved to: ../../data/merged\\fire_with_landcover.parquet\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "LATITUDE_CHUNK_SIZE = 5\n",
    "OVERLAP = 0.05\n",
    "LAT_MIN = 31.0\n",
    "LAT_MAX = 38.0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPATIAL JOIN: Fire Points â†’ Landcover Polygons\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Load landcover\n",
    "# -----------------------------\n",
    "print(\"\\n[1/4] Loading landcover data...\")\n",
    "landcover_df = pd.read_csv(land_file_path, usecols=['geometry', 'LCCCode'])\n",
    "print(f\"Landcover polygons: {len(landcover_df):,}\")\n",
    "\n",
    "# Convert WKT to shapely geometry\n",
    "landcover_df['geometry'] = landcover_df['geometry'].apply(wkt.loads)\n",
    "landcover_gdf = gpd.GeoDataFrame(landcover_df, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "# Build spatial index\n",
    "landcover_gdf.sindex\n",
    "del landcover_df\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Latitude bands\n",
    "# -----------------------------\n",
    "print(\"\\n[2/4] Creating latitude bands...\")\n",
    "lat_bands = []\n",
    "actual_min = np.floor(LAT_MIN * 100)/100\n",
    "actual_max = np.ceil(LAT_MAX * 100)/100\n",
    "current_lat = actual_min\n",
    "while current_lat < actual_max:\n",
    "    band_start = current_lat - OVERLAP\n",
    "    band_end = min(current_lat + LATITUDE_CHUNK_SIZE + OVERLAP, actual_max + OVERLAP)\n",
    "    lat_bands.append((band_start, band_end))\n",
    "    current_lat += LATITUDE_CHUNK_SIZE\n",
    "\n",
    "print(f\"Latitude bands: {len(lat_bands)}\")\n",
    "print(f\"Coverage: [{actual_min} - {actual_max}]\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Process each band\n",
    "# -----------------------------\n",
    "print(\"\\n[3/4] Processing latitude bands...\")\n",
    "start_time = datetime.now()\n",
    "results = []\n",
    "\n",
    "for i, (band_start, band_end) in enumerate(lat_bands, 1):\n",
    "    band_time = datetime.now()\n",
    "    print(f\"\\nBand {i}/{len(lat_bands)}: Lat [{band_start:.2f}, {band_end:.2f}]\", end='')\n",
    "\n",
    "    # Load fire points in band\n",
    "    fire_chunk = pd.read_parquet(\n",
    "        fire_file_path,\n",
    "        filters=[('latitude', '>=', band_start), ('latitude', '<=', band_end)]\n",
    "    )\n",
    "\n",
    "    if len(fire_chunk) == 0:\n",
    "        print(\" â†’ SKIP (no points)\")\n",
    "        continue\n",
    "\n",
    "    print(f\" â†’ {len(fire_chunk):,} pts\", end='')\n",
    "\n",
    "    # Convert to GeoDataFrame\n",
    "    fire_gdf = gpd.GeoDataFrame(\n",
    "        fire_chunk,\n",
    "        geometry=gpd.points_from_xy(fire_chunk.longitude, fire_chunk.latitude),\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "\n",
    "    # Filter landcover to bounding box\n",
    "    bounds = fire_gdf.total_bounds\n",
    "    landcover_filtered = landcover_gdf.cx[bounds[0]:bounds[2], bounds[1]:bounds[3]]\n",
    "    print(f\" | {len(landcover_filtered):,} polys\", end='')\n",
    "\n",
    "    if len(landcover_filtered) == 0:\n",
    "        print(\" â†’ WARNING: No polygons!\")\n",
    "        fire_chunk['LCCCode'] = pd.NA\n",
    "        results.append(fire_chunk[['longitude','latitude','fire','LCCCode']])\n",
    "        continue\n",
    "\n",
    "    # Spatial join using intersects\n",
    "    joined = gpd.sjoin(\n",
    "        fire_gdf,\n",
    "        landcover_filtered[['geometry','LCCCode']],\n",
    "        how='left',\n",
    "        predicate='intersects'\n",
    "    )\n",
    "\n",
    "    # Keep relevant columns and drop duplicates\n",
    "    result_chunk = joined[['longitude','latitude','fire','LCCCode']].drop_duplicates(subset=['longitude','latitude'])\n",
    "\n",
    "    # -----------------------------\n",
    "    # Nearest fallback for unmatched points\n",
    "    # -----------------------------\n",
    "    unmatched = result_chunk[result_chunk['LCCCode'].isna()]\n",
    "    if len(unmatched) > 0:\n",
    "        fire_unmatched_gdf = gpd.GeoDataFrame(\n",
    "            unmatched,\n",
    "            geometry=gpd.points_from_xy(unmatched.longitude, unmatched.latitude),\n",
    "            crs='EPSG:4326'\n",
    "        )\n",
    "        # Find nearest polygon for each unmatched point\n",
    "        nearest_idx = fire_unmatched_gdf.geometry.apply(lambda pt: landcover_filtered.distance(pt).idxmin())\n",
    "        result_chunk.loc[result_chunk['LCCCode'].isna(), 'LCCCode'] = landcover_filtered.loc[nearest_idx, 'LCCCode'].values\n",
    "\n",
    "    matched = result_chunk['LCCCode'].notna().sum()\n",
    "    print(f\" | âœ“ {matched:,} ({matched/len(result_chunk)*100:.1f}%)\", end='')\n",
    "\n",
    "    elapsed = (datetime.now() - band_time).total_seconds()\n",
    "    print(f\" | {elapsed:.1f}s\")\n",
    "\n",
    "    results.append(result_chunk)\n",
    "    del fire_chunk, fire_gdf, landcover_filtered, joined, result_chunk\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Combine all results\n",
    "# -----------------------------\n",
    "print(\"\\n[4/4] Combining all bands...\")\n",
    "final_result = pd.concat(results, ignore_index=True)\n",
    "print(f\"Before dedup: {len(final_result):,} rows\")\n",
    "final_result = final_result.drop_duplicates(subset=['longitude','latitude'])\n",
    "print(f\"After dedup: {len(final_result):,} rows\")\n",
    "\n",
    "# Save to parquet\n",
    "final_result.to_parquet(output_file, index=False, compression='snappy')\n",
    "\n",
    "# -----------------------------\n",
    "# Summary\n",
    "# -----------------------------\n",
    "total_time = (datetime.now() - start_time).total_seconds()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total fire points: {len(final_result):,}\")\n",
    "print(f\"With LCCCode: {final_result['LCCCode'].notna().sum():,}\")\n",
    "print(f\"Without LCCCode: {final_result['LCCCode'].isna().sum():,}\")\n",
    "print(f\"Match rate: {final_result['LCCCode'].notna().sum()/len(final_result)*100:.2f}%\")\n",
    "print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"Output saved to: {output_file}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cf7a444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fire\n",
       "0    89.558062\n",
       "1    10.441938\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result['fire'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "adae65d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b0a9d",
   "metadata": {},
   "source": [
    "# elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b95a6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NEAREST-NEIGHBOR JOIN: Fire + Elevation\n",
      "============================================================\n",
      "\n",
      "[1/3] Loading elevation data...\n",
      "   Elevation rows: 13,167,778\n",
      "   Building KD-Tree...\n",
      "   KD-Tree ready.\n",
      "\n",
      "[2/3] Processing fire data in chunks...\n",
      "\n",
      "   Chunk 1 â†’ Converting Arrow â†’ pandas...\n",
      "      Rows: 42,291\n",
      "      Added elevation. Max distance = 0.006047Â°\n",
      "      âœ“ Done in 0.2 seconds\n",
      "\n",
      "[3/3] Concatenating results and saving...\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Total points processed: 42,291\n",
      "Saved to: ../../data/merged\\fire_land_elev.parquet\n",
      "Total time: 0.0 minutes (0.00 hours)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Paths\n",
    "processed_data_dir = \"../../data/processed\"\n",
    "merged_dir = \"../../data/merged\"\n",
    "\n",
    "fire_land_file = os.path.join(merged_dir, \"fire_with_landcover.parquet\")\n",
    "elev_file_path = os.path.join(\"../../data/loaded\", \"elevation.csv\")\n",
    "output_file = os.path.join(merged_dir, \"fire_land_elev.parquet\")\n",
    "\n",
    "# Config\n",
    "CHUNK_SIZE = 3_000_000       # 3M safe chunk\n",
    "DECIMALS = 5                 # same precision as elevation dataset\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NEAREST-NEIGHBOR JOIN: Fire + Elevation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Load elevation table and build KD-Tree\n",
    "# ---------------------------------------------------\n",
    "print(\"\\n[1/3] Loading elevation data...\")\n",
    "\n",
    "elev = pd.read_csv(\n",
    "    elev_file_path,\n",
    "    usecols=[\"longitude\", \"latitude\", \"elevation\"]\n",
    ")\n",
    "\n",
    "print(f\"   Elevation rows: {len(elev):,}\")\n",
    "\n",
    "# Normalize lat/lon (avoid float noise mismatch)\n",
    "elev[\"longitude\"] = elev[\"longitude\"].round(DECIMALS)\n",
    "elev[\"latitude\"]  = elev[\"latitude\"].round(DECIMALS)\n",
    "\n",
    "# Build KD-Tree on (lat, lon)\n",
    "print(\"   Building KD-Tree...\")\n",
    "coords = np.vstack([elev[\"latitude\"].to_numpy(), elev[\"longitude\"].to_numpy()]).T\n",
    "kdtree = KDTree(coords, leaf_size=40)\n",
    "\n",
    "print(\"   KD-Tree ready.\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Read fire_land in chunks and query nearest elevation\n",
    "# ---------------------------------------------------\n",
    "print(\"\\n[2/3] Processing fire data in chunks...\")\n",
    "\n",
    "fire_pf = pq.ParquetFile(fire_land_file)\n",
    "results = []\n",
    "start_time = datetime.now()\n",
    "chunk_idx = 0\n",
    "\n",
    "for batch in fire_pf.iter_batches(batch_size=CHUNK_SIZE):\n",
    "    chunk_idx += 1\n",
    "    t0 = datetime.now()\n",
    "\n",
    "    print(f\"\\n   Chunk {chunk_idx} â†’ Converting Arrow â†’ pandas...\")\n",
    "    fire_chunk = batch.to_pandas()\n",
    "\n",
    "    print(f\"      Rows: {len(fire_chunk):,}\")\n",
    "\n",
    "    # Round coordinates\n",
    "    fire_chunk[\"longitude\"] = fire_chunk[\"longitude\"].round(DECIMALS)\n",
    "    fire_chunk[\"latitude\"]  = fire_chunk[\"latitude\"].round(DECIMALS)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Nearest elevation lookup\n",
    "    # -----------------------------------------------------------\n",
    "    fire_coords = np.vstack([fire_chunk[\"latitude\"], fire_chunk[\"longitude\"]]).T\n",
    "\n",
    "    dists, idx = kdtree.query(fire_coords, k=1)\n",
    "\n",
    "    # Get matching elevation values\n",
    "    fire_chunk[\"elevation\"] = elev[\"elevation\"].to_numpy()[idx[:, 0]]\n",
    "\n",
    "    print(f\"      Added elevation. Max distance = {dists.max():.6f}Â°\")\n",
    "\n",
    "    results.append(fire_chunk)\n",
    "\n",
    "    dt = (datetime.now() - t0).total_seconds()\n",
    "    print(f\"      âœ“ Done in {dt:.1f} seconds\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Combine and save\n",
    "# ---------------------------------------------------\n",
    "print(\"\\n[3/3] Concatenating results and saving...\")\n",
    "\n",
    "final_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "final_df.to_parquet(output_file, index=False, compression=\"snappy\")\n",
    "\n",
    "total_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total points processed: {len(final_df):,}\")\n",
    "print(f\"Saved to: {output_file}\")\n",
    "print(f\"Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e64d7bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fire\n",
       "0    89.558062\n",
       "1    10.441938\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['fire'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90354ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42291, 5)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d2c95",
   "metadata": {},
   "source": [
    "# Soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1242cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Loading soil data...\n",
      "   Soil rows: 3,291,933\n",
      "\n",
      "[2/3] Preparing latitude bands...\n",
      "   Total bands: 1\n",
      "\n",
      "[3/3] Processing...\n",
      "\n",
      "   Band 1/1 â†’ [29.95, 38.05]\n",
      "      Fire pts: 42,291\n",
      "      âœ“ matched & dedup: 42,291\n",
      "\n",
      "Combining all bands...\n",
      "\n",
      "Final rows: 42,291\n",
      "Columns: ['longitude', 'latitude', 'fire', 'LCCCode', 'elevation', 'soil_lon', 'soil_lat', 'COARSE', 'SAND', 'CLAY', 'TCARBON_EQ', 'PH_WATER', 'TOTAL_N', 'CN_RATIO', 'CEC_SOIL', 'ESP', 'GYPSUM']\n",
      "Saved â†’ ../../data/merged\\fire_land_elev_soil.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# ------------ MEMORY CLEANUP ----------------\n",
    "def free_mem(*vars):\n",
    "    for v in vars:\n",
    "        try: del v\n",
    "        except: pass\n",
    "    gc.collect()\n",
    "\n",
    "# ------------ PATHS --------------------------\n",
    "data_dir = \"../../data/loaded\"\n",
    "processed_data_dir = \"../../data/processed\"\n",
    "merged_dir = \"../../data/merged\"\n",
    "Path(merged_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fire_file_path = os.path.join(merged_dir, \"fire_land_elev.parquet\")\n",
    "soil_file_path = os.path.join(processed_data_dir, \"soil_reduced_nondup.csv\")\n",
    "out_file = os.path.join(merged_dir, \"fire_land_elev_soil.parquet\")\n",
    "\n",
    "# ------------ PARAMETERS ----------------------\n",
    "LAT_CHUNK = 10\n",
    "OVERLAP = 0.05\n",
    "LAT_MIN = 30.00000\n",
    "LAT_MAX = 38.00000\n",
    "\n",
    "soil_cols = [\n",
    "    'longitude', 'latitude', 'COARSE', 'SAND', 'CLAY',\n",
    "    'TCARBON_EQ', 'PH_WATER', 'TOTAL_N', 'CN_RATIO',\n",
    "    'CEC_SOIL', 'ESP', 'GYPSUM'\n",
    "]\n",
    "\n",
    "# ------------ LOAD SOIL -----------------------\n",
    "print(\"[1/3] Loading soil data...\")\n",
    "soil = pd.read_csv(soil_file_path, usecols=soil_cols)\n",
    "\n",
    "# Rename soil longitude/latitude to avoid duplicates\n",
    "soil = soil.rename(columns={'longitude':'soil_lon', 'latitude':'soil_lat'})\n",
    "\n",
    "# Keep 5-decimal precision\n",
    "soil['soil_lon'] = soil['soil_lon'].round(5)\n",
    "soil['soil_lat'] = soil['soil_lat'].round(5)\n",
    "\n",
    "soil_points = soil[['soil_lon','soil_lat']].to_numpy()\n",
    "tree = cKDTree(soil_points)\n",
    "print(f\"   Soil rows: {len(soil):,}\")\n",
    "\n",
    "# ---------- LATITUDE BANDS --------------------\n",
    "print(\"\\n[2/3] Preparing latitude bands...\")\n",
    "bands = []\n",
    "cur = np.floor(LAT_MIN * 100) / 100\n",
    "lat_end = np.ceil(LAT_MAX * 100) / 100\n",
    "\n",
    "while cur < lat_end:\n",
    "    bands.append((cur - OVERLAP,\n",
    "                  min(cur + LAT_CHUNK + OVERLAP, lat_end + OVERLAP)))\n",
    "    cur += LAT_CHUNK\n",
    "\n",
    "print(f\"   Total bands: {len(bands)}\")\n",
    "\n",
    "# ------------- PROCESS BANDS -------------------\n",
    "results = []\n",
    "\n",
    "print(\"\\n[3/3] Processing...\")\n",
    "for idx, (b_start, b_end) in enumerate(bands,1):\n",
    "\n",
    "    print(f\"\\n   Band {idx}/{len(bands)} â†’ [{b_start:.2f}, {b_end:.2f}]\")\n",
    "\n",
    "    fire_chunk = pd.read_parquet(\n",
    "        fire_file_path,\n",
    "        filters=[('latitude','>=',b_start),('latitude','<=',b_end)]\n",
    "    )\n",
    "    if len(fire_chunk)==0:\n",
    "        print(\"      skip (no points)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"      Fire pts: {len(fire_chunk):,}\")\n",
    "\n",
    "    # Round to 5 decimals for stability\n",
    "    fire_chunk['longitude'] = fire_chunk['longitude'].round(5)\n",
    "    fire_chunk['latitude'] = fire_chunk['latitude'].round(5)\n",
    "\n",
    "    fire_pts = fire_chunk[['longitude','latitude']].to_numpy()\n",
    "\n",
    "    # Query KDTree for nearest soil point\n",
    "    dist, idxs = tree.query(fire_pts, k=1)\n",
    "\n",
    "    soil_match = soil.iloc[idxs].reset_index(drop=True)\n",
    "\n",
    "    # Merge fire + nearest soil (soil coords now renamed)\n",
    "    merged = pd.concat([fire_chunk.reset_index(drop=True),\n",
    "                        soil_match.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # --- DROP DUPLICATES PER BAND ---\n",
    "    merged = merged.drop_duplicates(subset=['longitude','latitude'])\n",
    "\n",
    "    print(f\"      âœ“ matched & dedup: {len(merged):,}\")\n",
    "\n",
    "    results.append(merged)\n",
    "\n",
    "    free_mem(fire_chunk, fire_pts, soil_match, merged)\n",
    "\n",
    "# ------------- FINAL MERGE ---------------------\n",
    "print(\"\\nCombining all bands...\")\n",
    "final = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Final deduplication (should be minimal now)\n",
    "final = final.drop_duplicates(subset=['longitude','latitude'])\n",
    "\n",
    "print(f\"\\nFinal rows: {len(final):,}\")\n",
    "print(f\"Columns: {final.columns.tolist()}\")\n",
    "\n",
    "final.to_parquet(out_file, index=False, compression='snappy')\n",
    "print(f\"Saved â†’ {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e55ca1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fire\n",
       "0    89.558062\n",
       "1    10.441938\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['fire'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d767e2",
   "metadata": {},
   "source": [
    "# climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a397873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Fire columns: ['longitude', 'latitude', 'fire', 'LCCCode', 'elevation', 'soil_lon', 'soil_lat', 'COARSE', 'SAND', 'CLAY', 'TCARBON_EQ', 'PH_WATER', 'TOTAL_N', 'CN_RATIO', 'CEC_SOIL', 'ESP', 'GYPSUM']\n",
      "ðŸŒ¦ Climate columns: ['lon', 'lat', 'prec_median_autumn', 'prec_median_spring', 'prec_median_summer', 'prec_median_winter', 'tmin_median_autumn', 'tmin_median_spring', 'tmin_median_summer', 'tmin_median_winter', 'tmax_median_autumn', 'tmax_median_spring', 'tmax_median_summer', 'tmax_median_winter', 'prec_iqr_autumn', 'prec_iqr_spring', 'prec_iqr_summer', 'prec_iqr_winter', 'tmin_iqr_autumn', 'tmin_iqr_spring', 'tmin_iqr_summer', 'tmin_iqr_winter', 'tmax_iqr_autumn', 'tmax_iqr_spring', 'tmax_iqr_summer', 'tmax_iqr_winter', 'tmax_max', 'prec_min', 'longest_dry_period', 'longest_hot_period']\n",
      "âš  Columns that would overlap: set()\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "merged_dir = \"../../data/merged\"\n",
    "processed_data_dir = \"../../data/processed\"\n",
    "\n",
    "fire_file_path = os.path.join(merged_dir, \"fire_land_elev_soil.parquet\")\n",
    "clim_file_path = os.path.join(processed_data_dir, \"processed_climate.csv\")\n",
    "\n",
    "# Get Parquet column names without loading full data\n",
    "pf = pq.ParquetFile(fire_file_path)\n",
    "fire_cols = pf.schema.names\n",
    "\n",
    "# Get CSV column names (very cheap)\n",
    "clim_cols = pd.read_csv(clim_file_path, nrows=0).columns.tolist()\n",
    "\n",
    "print(\"ðŸ”¥ Fire columns:\", fire_cols)\n",
    "print(\"ðŸŒ¦ Climate columns:\", clim_cols)\n",
    "\n",
    "# Check for overlap\n",
    "overlap = set(fire_cols) & set(clim_cols)\n",
    "print(\"âš  Columns that would overlap:\", overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc621927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Loading data...\n",
      " Fire shape before: (42291, 17)\n",
      " Climate shape: (32927, 30)\n",
      "[2/3] Building KDTree for climate...\n",
      " Querying nearest climate points...\n",
      " Distance summary:\n",
      "   min: 0.0009716995420378196\n",
      "   max: 0.2818213121820252\n",
      "   mean: 0.031619546642710813\n",
      "[3/3] Merging...\n",
      " Dedup: before=42,291 â†’ after=42,291\n",
      "\n",
      "Saved â†’ ../../data/merged\\fire_land_elev_soil_climate.parquet\n",
      "Final shape: (42291, 47)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "import os\n",
    "import gc\n",
    "\n",
    "def free_mem(*vars):\n",
    "    for v in vars:\n",
    "        try: del v\n",
    "        except: pass\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# ------------------- PATHS -----------------------\n",
    "merged_dir = \"../../data/merged\"\n",
    "processed_data_dir = \"../../data/processed\"\n",
    "os.makedirs(merged_dir, exist_ok=True)\n",
    "\n",
    "fire_file_path = os.path.join(merged_dir, \"fire_land_elev_soil.parquet\")\n",
    "clim_file_path = os.path.join(processed_data_dir, \"processed_climate.csv\")\n",
    "out_file = os.path.join(merged_dir, \"fire_land_elev_soil_climate.parquet\")\n",
    "\n",
    "# -------------------- LOAD DATA -------------------\n",
    "print(\"[1/3] Loading data...\")\n",
    "\n",
    "fire = pd.read_parquet(fire_file_path)\n",
    "clim = pd.read_csv(clim_file_path)\n",
    "\n",
    "print(\" Fire shape before:\", fire.shape)\n",
    "print(\" Climate shape:\", clim.shape)\n",
    "\n",
    "# Round fire coords\n",
    "fire[\"longitude\"] = fire[\"longitude\"].round(5)\n",
    "fire[\"latitude\"] = fire[\"latitude\"].round(5)\n",
    "\n",
    "# Prepare climate\n",
    "clim = clim.rename(columns={'lon':'clim_lon','lat':'clim_lat'})\n",
    "clim[\"clim_lon\"] = clim[\"clim_lon\"].round(5)\n",
    "clim[\"clim_lat\"] = clim[\"clim_lat\"].round(5)\n",
    "\n",
    "clim_cols_extra = [c for c in clim.columns if c not in ['clim_lon','clim_lat']]\n",
    "\n",
    "# Build KDTree\n",
    "print(\"[2/3] Building KDTree for climate...\")\n",
    "clim_points = clim[['clim_lon','clim_lat']].to_numpy()\n",
    "tree = cKDTree(clim_points)\n",
    "\n",
    "# Query\n",
    "print(\" Querying nearest climate points...\")\n",
    "fire_pts = fire[['longitude','latitude']].to_numpy()\n",
    "\n",
    "dist, idxs = tree.query(fire_pts, k=1)\n",
    "\n",
    "print(\" Distance summary:\")\n",
    "print(\"   min:\", float(dist.min()))\n",
    "print(\"   max:\", float(dist.max()))\n",
    "print(\"   mean:\", float(dist.mean()))\n",
    "\n",
    "# Build climate match dataframe\n",
    "clim_match = clim.iloc[idxs].reset_index(drop=True)\n",
    "\n",
    "# Merge\n",
    "print(\"[3/3] Merging...\")\n",
    "merged = pd.concat([fire.reset_index(drop=True), clim_match], axis=1)\n",
    "\n",
    "# Drop duplicate fire coords\n",
    "before = merged.shape[0]\n",
    "merged = merged.drop_duplicates(subset=['longitude','latitude'])\n",
    "after = merged.shape[0]\n",
    "\n",
    "print(f\" Dedup: before={before:,} â†’ after={after:,}\")\n",
    "\n",
    "# Save\n",
    "merged.to_parquet(out_file, index=False, compression='snappy')\n",
    "print(f\"\\nSaved â†’ {out_file}\")\n",
    "print(\"Final shape:\", merged.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "366fbcb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fire\n",
       "0    89.558062\n",
       "1    10.441938\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged['fire'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f8167fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42291, 47)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716738d2",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c25f890e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42291, 47)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check duplicates in final file\n",
    "import pandas as pd\n",
    "import os\n",
    "merged_dir = \"../../data/merged\"\n",
    "fire_file_path = os.path.join(merged_dir, \"fire_land_elev_soil_climate.parquet\")\n",
    "fire_cropped = pd.read_parquet(fire_file_path)\n",
    "fire_cropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fe6e718a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42291, 47)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ea59d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(78136)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check duplicated lon/lat\n",
    "duplicates = fire_cropped.duplicated(subset=['longitude','latitude'])\n",
    "duplicates.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ba1f1bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42291, 47)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicates by takin first occurrence\n",
    "fire_cropped = fire_cropped.drop_duplicates(subset=['longitude','latitude'], keep='first')\n",
    "fire_cropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6fe46f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longitude', 'latitude', 'fire', 'LCCCode', 'elevation', 'soil_lon',\n",
       "       'soil_lat', 'COARSE', 'SAND', 'CLAY', 'TCARBON_EQ', 'PH_WATER',\n",
       "       'TOTAL_N', 'CN_RATIO', 'CEC_SOIL', 'ESP', 'GYPSUM', 'clim_lon',\n",
       "       'clim_lat', 'prec_median_autumn', 'prec_median_spring',\n",
       "       'prec_median_summer', 'prec_median_winter', 'tmin_median_autumn',\n",
       "       'tmin_median_spring', 'tmin_median_summer', 'tmin_median_winter',\n",
       "       'tmax_median_autumn', 'tmax_median_spring', 'tmax_median_summer',\n",
       "       'tmax_median_winter', 'prec_iqr_autumn', 'prec_iqr_spring',\n",
       "       'prec_iqr_summer', 'prec_iqr_winter', 'tmin_iqr_autumn',\n",
       "       'tmin_iqr_spring', 'tmin_iqr_summer', 'tmin_iqr_winter',\n",
       "       'tmax_iqr_autumn', 'tmax_iqr_spring', 'tmax_iqr_summer',\n",
       "       'tmax_iqr_winter', 'tmax_max', 'prec_min', 'longest_dry_period',\n",
       "       'longest_hot_period'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_cropped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c2728651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove cols: soil_lon, soil_lat, clim_lon, clim_lat\n",
    "fire_cropped = fire_cropped.drop(columns=['soil_lon', 'soil_lat', 'clim_lon', 'clim_lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "28ff0406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>fire</th>\n",
       "      <th>LCCCode</th>\n",
       "      <th>elevation</th>\n",
       "      <th>COARSE</th>\n",
       "      <th>SAND</th>\n",
       "      <th>CLAY</th>\n",
       "      <th>TCARBON_EQ</th>\n",
       "      <th>PH_WATER</th>\n",
       "      <th>...</th>\n",
       "      <th>tmin_iqr_summer</th>\n",
       "      <th>tmin_iqr_winter</th>\n",
       "      <th>tmax_iqr_autumn</th>\n",
       "      <th>tmax_iqr_spring</th>\n",
       "      <th>tmax_iqr_summer</th>\n",
       "      <th>tmax_iqr_winter</th>\n",
       "      <th>tmax_max</th>\n",
       "      <th>prec_min</th>\n",
       "      <th>longest_dry_period</th>\n",
       "      <th>longest_hot_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.68184</td>\n",
       "      <td>33.53246</td>\n",
       "      <td>1</td>\n",
       "      <td>0011</td>\n",
       "      <td>183</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>45.333333</td>\n",
       "      <td>20.333333</td>\n",
       "      <td>9.433333</td>\n",
       "      <td>7.866667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.750</td>\n",
       "      <td>5.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.625</td>\n",
       "      <td>1.50</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.27448</td>\n",
       "      <td>33.45376</td>\n",
       "      <td>1</td>\n",
       "      <td>6004</td>\n",
       "      <td>102</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1.500</td>\n",
       "      <td>5.250</td>\n",
       "      <td>3.875</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1.50</td>\n",
       "      <td>42.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.53867</td>\n",
       "      <td>33.36744</td>\n",
       "      <td>1</td>\n",
       "      <td>6004</td>\n",
       "      <td>21</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>44.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.35868</td>\n",
       "      <td>35.69833</td>\n",
       "      <td>1</td>\n",
       "      <td>0003 / 0004</td>\n",
       "      <td>865</td>\n",
       "      <td>19.666667</td>\n",
       "      <td>50.333333</td>\n",
       "      <td>21.333333</td>\n",
       "      <td>11.033333</td>\n",
       "      <td>7.633333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.875</td>\n",
       "      <td>0.375</td>\n",
       "      <td>4.375</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.625</td>\n",
       "      <td>1.00</td>\n",
       "      <td>35.75</td>\n",
       "      <td>1.600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.53032</td>\n",
       "      <td>34.93820</td>\n",
       "      <td>1</td>\n",
       "      <td>0011</td>\n",
       "      <td>752</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>4.750</td>\n",
       "      <td>4.625</td>\n",
       "      <td>1.625</td>\n",
       "      <td>1.25</td>\n",
       "      <td>37.25</td>\n",
       "      <td>1.075</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  fire      LCCCode  elevation     COARSE       SAND  \\\n",
       "0    9.68184  33.53246     1         0011        183   9.000000  45.333333   \n",
       "1    9.27448  33.45376     1         6004        102   3.000000  37.500000   \n",
       "2    8.53867  33.36744     1         6004         21   6.000000  90.000000   \n",
       "3    8.35868  35.69833     1  0003 / 0004        865  19.666667  50.333333   \n",
       "4    8.53032  34.93820     1         0011        752   3.000000  55.000000   \n",
       "\n",
       "        CLAY  TCARBON_EQ  PH_WATER  ...  tmin_iqr_summer  tmin_iqr_winter  \\\n",
       "0  20.333333    9.433333  7.866667  ...            1.500            0.750   \n",
       "1  22.000000   16.250000  8.000000  ...            1.500            1.500   \n",
       "2   5.000000    0.000000  6.700000  ...            1.500            1.250   \n",
       "3  21.333333   11.033333  7.633333  ...            1.875            0.375   \n",
       "4  18.000000    2.000000  7.500000  ...            1.625            0.625   \n",
       "\n",
       "   tmax_iqr_autumn  tmax_iqr_spring  tmax_iqr_summer  tmax_iqr_winter  \\\n",
       "0            5.000            4.000            1.625             1.50   \n",
       "1            5.250            3.875            1.500             1.50   \n",
       "2            5.000            4.000            2.000             2.00   \n",
       "3            4.375            4.000            1.625             1.00   \n",
       "4            4.750            4.625            1.625             1.25   \n",
       "\n",
       "   tmax_max  prec_min  longest_dry_period  longest_hot_period  \n",
       "0     40.00     0.000                   1                   2  \n",
       "1     42.00     0.000                   1                   3  \n",
       "2     44.00     0.000                   2                   3  \n",
       "3     35.75     1.600                   0                   0  \n",
       "4     37.25     1.075                   0                   0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_cropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f8c7809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save again \n",
    "fire_cropped.to_parquet(fire_file_path, index=False, compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f2cf500d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fire\n",
       "0    89.558062\n",
       "1    10.441938\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_cropped['fire'].value_counts(normalize=True) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
